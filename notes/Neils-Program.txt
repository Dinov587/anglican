Hi,

I helped Neil to prepare the Anglican program for inferring
motions for running, and to run it. I also made the program
and the embedded anglican implementation meet --- slightly
modified the program, as well as extended the implementation
with matrix stuff, multivariate distributions, and
random processes.

The program is at:

https://bitbucket.org/fwood/anglican/src/46b8c3cdec0395e36db1771ae801fb76b8e67594/experiments/sticky_HPD_HMM_KS_v1.anglican?at=Neil_Locomotion

I want to share my experiences, some of which are trivial
for more experienced Anglican users, but some may be useful
for further improvements.

First, some numbers. I finally started the program properly
on laura yesterday, 15 hours ago. I have 30 processes running,
10 each of rdb, pgibbs, and cascade each. So far, 
                  
                 |  rdb     |     pgibbs    |    cascade
Produced samples |  41736   |     113600    |       0
Unique samples   |    886   |         67    |       0

Cascade does not produce any output, for some reason; I haven't
investigated yet. pgibbs is roughly 2.5 times faster than rdb,
but we know the reason --- rdb runs twice as many samples, the
extra sample to compute conditional transition probability from
the new to the old particle, which is absolutely unnecessary, and
can be fixed easily, as we discussed with Yura, and even have a 
(low priority) issue on this.

The `acceptance rate' is much higher for rdb (which is to be
expected), 2% for rdb versus 0.06% for pgibbs. I haven't tried
pgibbs with ancestral resampling, because it does not seem to
be in the branch. Should I?

I also made changes to m! (and to the program) to make the former
run the later. The program with only minimal changes is here:

https://bitbucket.org/dtolpin/embang/src/HEAD/code/src/angsrc/hdp_hmm_ks_original.clj

Except for some formatting and renaming (I don't like using
overloaded arithmetic operators for matrices and prefer writing
function names: add, sub, mul etc), the differences are

lines 12-16,

   where a CRP-based source for activity indices is defined.
   In m!, crp (and a random process is general) is still immutable, and
   has method `advance' (in addition to observe and sample), which
   produces the next state of the random process. In addition, an m!
   program has a global `store' (it is a state monad) to which values,
   indexed by some key, can be stored and from which they can be
   retrieved. Hence, line 13 retrieves the current state of CRP,
   line 15 stores the updated state. I believe that this abstraction
   is useful for implementing random processes without having to
   hard-code state manipulations.

lines 102-116,

   is a loop over all fixed activity data sets. It does exactly the
   same as the unrolled loop of observations in the original code,
   but in my opinion is slighlty better structured. The data itself
   is in a vector in a separate module (angsrc.hdp-hmm-ks-data).

line 119-126

   the same for varying activity dataset.


I haven't implemented RDB yet, so can't really get useful inference
results (but if decide to, I'd estimate it as a one day effort ---
but may actually be 2 weeks if we use the rule `bump up the unit and
multiply by \pi). Still, I could run the inference with Partical Gibbs
and compare the performance.

To be continued.
